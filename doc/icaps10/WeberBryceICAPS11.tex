\documentclass[letterpaper]{article}

\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
%%%%%%%%%%
% PDFMARK for TeX and GhostScript
% Uncomment and complete the following for
% metadata if your paper is typeset using TeX and
% GhostScript (e.g if you use .ps or .eps files in your paper):
\special{! /pdfmark where
{pop} {userdict /pdfmark /cleartomark load put} ifelse
[ /Author (Christopher Weber, Daniel Bryce)
 /Title (Planning and Acting in Incomplete Domains)
 /Subject (International Conference on Automated Planning and Scheduling)
 /Keywords (Planning, Execution, Learning, Uncertainty)
 /DOCINFO pdfmark
} 
%%%%%%%%%%
\setcounter{secnumdepth}{1}

\usepackage{graphicx}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{multirow}
\usepackage{amsmath, amsthm, amssymb}
%\usepackage{natbib,natbibspacing}

% \renewcommand\floatpagefraction{1}
% \renewcommand\topfraction{1} \renewcommand\bottomfraction{1} \renewcommand\textfraction{0}
% \renewcommand{\dbltopfraction}{1} \renewcommand{\dblfloatpagefraction}{1}
% \setcounter{totalnumber}{50} \setcounter{topnumber}{50}
% \setcounter{bottomnumber}{50} \setcounter{dbltopnumber}{50}
%\setlength{\bibspacing}{0pt}
\newenvironment{packed_enum}{
\begin{enumerate}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{enumerate}}
\newenvironment{packed_itemize}{
\begin{itemize}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}


%\def\citet#1{\citeA{#1}} 
\def\und#1{\noindent{\bf #1}:}
%\def\baselinestretch{.99}
\def\FFRISKY{{\tt DeFAULT}}
\def\default{{\tt DeFAULT}} 
\def\goalie{{\tt Goalie}}
\def\citep#1{\cite{#1}} 
\def\citet#1{\citeauthor{#1} (\citeyear{#1})}




\begin{document}
\title{Planning and Acting in Incomplete Domains}

\author{
Christopher Weber \and Daniel Bryce\\
christopherweber@hotmail.com, daniel.bryce@usu.edu\\
Department of Computer Science\\
Utah State Univeristy
}

\maketitle

\begin{abstract}
Engineering complete planning domain descriptions is often very costly because
of human error or lack of domain knowledge. Learning complete domain
descriptions is also very challenging because many features are irrelevant to
achieving the goals and data may be scarce.  We present an agent that plans and
acts in incomplete domains by i) synthesizing plans to avoid execution failure
due to ignorance of the domain model, and ii) passively learning about the
domain model during execution to improve later re-planning attempts.

Our planner \default{} is the first to reason about a domain's incompleteness to
avoid potential plan failure.  \default{} computes failure explanations for each
action and state in the plan and counts the number of interpretations of the
incomplete domain where failure will occur. We show that \default{} performs
best by counting  prime implicants (failure diagnoses) rather than propositional
models. Our agent \goalie{} learns about the preconditions and effects of
incompletely-specified actions while monitoring its state and, in conjunction
with \default{} plan failure explanations, can diagnose past and future action
failures.   We show that by reasoning about incompleteness in planning (as
opposed to ignoring it) \goalie{} re-plans less, executes fewer actions, and
achieves its goals faster.

\end{abstract}


\section{Introduction}
%    %Planning with incomplete domains is important in applications (incl BL)
The knowledge engineering required to create complete and correct domain
descriptions for planning problems is often very costly and difficult
\citep{modellite,arms}.  Machine learning techniques have been applied with some
success \citep{arms}, but still suffer from impoverished data and limitations of
the algorithms \citep{modellite}.   In particular, we are motivated by
applications in instructable computing \citep{mable} where a domain expert
teaches an intelligent system about a domain, but can often leave out whole
procedures (plans) and aspects of action descriptions.   In such cases, the
alternative to making domains complete, is to plan around the incompleteness. 
That is, given knowledge of the possible action descriptions, we seek out plans
that will succeed despite any (or most) incompleteness in the domain
formulation.
%  %Prior work does not look at synthesis, just computes critical faults

While prior work \citep{Garland02} (henceforth abbreviated, GL) has categorized
risks to a plan and described plan quality metrics in terms of the risks
(essentially single-fault diagnoses of plan failure \citep{dekleer}), no
prior work has sought to deliberately synthesize low-risk plans based on
incomplete STRIPS-style domains (notable work in Markov decision processes
\citep{NE:05}  and model-based reinforcement learning \citep{citeulike:112017}
has explored similar issues).  Our planner \default{} labels parital
plans with propositional explanations of failure due to incompleteness (derived
from the semantics of assumption based truth maintenance systems
\cite{USU-CS-TR-11-001}) and either counts failure models or prime implicants (diagnoses) to bias search.
% Specifically, \citet{Garland02}  identify four types of plan faults: open
% preconditions (due to incomplete preconditions), possible clobberers (due to
% incomplete delete effects), unlisted effects (due to incomplete add effects),
% and false preconditions.  GL develop an algorithm that steps backward through
% the plan to identify the ``critical faults'' -- those instances where
% incomplete domain features can cause plan failure.  For example, a possible
% clobber is a critical fault when (if it is truly a delete effect) it threatens
% a precondition or goal.  The number of critical faults is an important measure
% of plan quality/correctness, that, unfortunately, no known planners seek to
% minimize.
Our agent \goalie{} passively learns about the incomplete domain at it
executes actions, like \citet{DBLP:conf/aips/ChangA06} (henceforth
abbreviated, CA); however, unlike CA, \goalie{} executes plans that are robust
to domain incompleteness.  Within \goalie{}, we compare the use of robust plans
generated by our planner \default{}, and plans that are generated in the
spirit of CA which are not intentionally robust (i.e., optimistically
successful).  We demonstrate that the additional effort to synthesize robust
plans is justified because \default{} executes fewer overall actions, re-plans
less, and solves tasks in lower overall time.

% 
%  CA synthesize plans
% that will achieve the goals under any interpretation of the incomplete domain,
% however in doing so the planner can pick the most convenient interpretation and
% be overly optimistic.   Further, while operating in non-deterministic domains,
% they do not address learning action preconditions and assume that their
% environment will provide action failure signals.  We focus on deterministic
% environments, but lift their assumptions by learning preconditions and not
% relying on the environment signaling failure.

% Our planner \FFRISKY{} builds upon GL to synthesize plans robust to
% incompleteness and our agent \goalie{} builds upon CA to use robust plans while
% learning about all types of incomplete action features and diagnosing plan
% failure.  We rely on representing our knowledge of the incomplete domain and
% anticipated plan failure with propositional sentences that are manipulated
% during plan synthesis and execution.  The advantage of reasoning about the
% incompleteness in this fashion is that we can potentially avoid plan failure
% (i.e., unsatisfied (sub)goals), minimize re-planning, and often reduce overall
% planning and execution time over approaches that do not reason about
% incompleteness, such as the planner used by CA.

This paper is organized as follows.  The next section details Incomplete STRIPS,
the language we use to describe incomplete domains.  We follow with our approach
to plan synthesis and search heuristics.  We discuss alternatives to reasoning
about failure explanations, including model counting and prime implicant
counting.  We describe our execution monitoring and re-planning strategy, and
then provide an empirical analysis, related work, and conclusion.
  

%
%\und{Example} Consider the following action, that is taken from a modified version of the International Planning Competition (IPC) \citep{ipc5} PARC printer domain:  
%
%\begin{verbatim}
%(:action HtmOverBlack-Move-A4
% :parameters ( ?sheet - sheet_t )
% :precondition (and (clear) (Available HtmOverBlack-RSRC)
%                    (Sheetsize ?sheet A4) 
%                    (Location ?sheet HtmOverBlack_Entry-EndCap_Exit))
% :effect (and (not (Available HtmOverBlack-RSRC))
%              (Location ?sheet HtmOverBlack_Exit-Down_TopEntry)
%              (not (Location ?sheet HtmOverBlack_Entry-EndCap_Exit))
%              (Available HtmOverBlack-RSRC))
% :poss-effect (and (not (clear))))
%\end{verbatim}
%
%\noindent The action models a modular printer component that prints on a sheet of A4-sized paper.  The action is incomplete because it has a possible effect that the component will become jammed ``{\tt (not (clear))}''.  The intuition behind the action is that the component manufacturer did not provide complete specifications and it is unknown if feeding an A4 sheet will cause a paper jam.  Note that an incomplete action is different from a non-deterministic action because each application of the incomplete action will have the same effect at runtime; however, it is not clear what the effect will be at planning time.  The action incompleteness can cause plan failure, as in the case of our example, by threatening the precondition of a later action (e.g., the precondition {\tt (clear)} is threatened in a second application of the {\tt HtmOverBlack-Move-A4} action).  
%
%\und{Interpretations of Incompleteness} A pessimistic approach to reasoning about incomplete actions might assume that possible delete effects will always occur.  Plans found under this pessimistic interpretation will be correct despite any action incompleteness, but are likely to be few or nonexistent.  In the PARC printer example, a pessimistic interpretation will likely lead to proving that no plan exists, even though it is possible that the action does not have the delete effect on {\tt (clear)}.  Alternatively, an optimistic interpretation might assume that no possible delete effect occurs, in which case the planner can ignore that {\tt (clear)} may be deleted.  The optimistic interpretation is equally flawed because the action may actually delete {\tt (clear)}.  Instead, we adopt a cautiously optimistic interpretation where, like the optimistic interpretation, we assume that possible delete effects do not occur, but we also temper our optimism.  We compute an explanation for cases under which each proposition that is optimistically true might be false.  For example, after applying the action above, we would assert that {\tt (clear)} is true, subject to the assumption that {\tt (clear)} is not a delete effect of the action.  Under this cautiously optimistic semantics, we can determine which interpretations of incomplete actions will result in failed goal achievement by inspecting the assumptions under which the goals are false.  Plans that fail under fewer interpretations are preferred.
%
%
%\und{Failure Explanations and Counting} We take three qualitatively different approaches to recording a failure explanation for each propositions established at different times by a plan.  The first, our control, amounts to the optimistic interpretation by recording no explanation for the failure to achieve a proposition.  The second and third approaches represent failure explanations with propositional sentences, whose models correspond to interpretations of the incomplete actions.   The second approach relies on intuitions from model-based diagnosis to represent each failure explanation by a set of diagnoses (each diagnosis is a conjunction of incomplete action features -- i.e., a prime implicant).   The third approach represents failure explanations by OBDDs.  The second and third approaches provide a representation suitable for counting interpretations of the incomplete action features (i.e., propositional models) under which a proposition is achieved or not.  The primary difference is that model counting with prime implicants is intractable \citep{Roth96},  but polynomial in the size of an OBDD \citep{darwiche}.  While we use each of the three approaches during plan synthesis to compare plans (in varying capacities), we use the third to provide a final assessment of a plan's quality: the number of interpretations of the incomplete actions under which the plan fails.  That is, we describe several heuristic techniques to speed-up plan synthesis that are based on a particular representation of the failure explanations, but compare the resulting plans with a single, non-heuristic method.
%
%For example, the first approach is entirely heuristic because it completely ignores failure explanations.  In the second approach, we represent the failure explanations by prime implicants and, instead of counting models, we count the number of prime implicants.  Counting prime implicants is a computationally inexpensive heuristic that assumes fewer diagnoses means fewer failed interpretations of the incomplete actions.  The third method counts the actual number of failed action interpretations by representing them as an OBDD (which can be exponential-sized) and performing OBDD model counting (which is polynomial in the OBDD size).
%%in the first approach where we do not record justifications there is nothing to count during plan synthesis, but our post-synthesis plan validator can count interpretations.  In the second approach, we count the number of diagnoses to bias the plan synthesis, but again use the post-synthesis plan validator to measure the plan quality.  In the third approach, we count models of the OBDDs to bias the plan synthesis (which also forms the basis for our plan validator).  
%We claim that counting diagnoses (prime implicants) is more computationally feasible than counting OBDD models and the resulting plans are of similar quality, and that ignoring incompleteness altogether leads to poor quality plans.
%
%Our claims are based upon GL's focus on counting a plan's critical risks as a measure of its quality.  We observe that GL's definition of critical risks is equivalent to computing single-fault diagnoses, which allows us to generalize their notions to multi-fault diagnoses.  Intuitively, with more diagnoses for plan failure, the fewer interpretations of the incomplete domain will achieve the goal.  Naturally, a single-fault diagnosis covers more interpretations than a double- or triple-fault, so we count not just the number of diagnoses, but those of different cardinality.  We stress that counting diagnoses is an approximation to counting models, but it nevertheless leads to more efficient planners that find comparable quality solutions.
%
%
%
%%We do forward heuristic search, which requires carrying forward possible faults
%%We propagate faults on a planning graph
%
%\und{Planners} We present a forward heuristic planner, called \FFRISKY{}, that propagates failure explanations in the state space and relaxed planning problems.  \FFRISKY{} associates a set of explanations with each time step (i.e., each state in the search space or each planning graph layer in the relaxed planning problem).  \FFRISKY{}'s heuristic biases search toward plans that will fail in the fewest interpretations of the incomplete domain as possible.
%%use several sources of support for subgoals (similar to the intuition behind CNLP \citep{peot92conditional}).  Using multiple supporters can decrease the number of incomplete domain interpretations that fail to achieve a subgoal because, in most cases, the supporters are susceptible to different faults.  However, using too many supporters increases plan length.  \FFRISKY{} reasons about which and how many supporters to use while constructing its heuristic.  
%Because no prior work exists for the purpose of empirical comparisons, we not only compare \FFRISKY{} with a planner that uses the FF heuristic and ignores domain incompleteness, but also attempt a {\em more fair} comparison with a conformant probabilistic planner.  
%
%%We demonstrate on some modified IPC problems how it works
%
%Our results indicate that \FFRISKY{} can find much better quality plans than a planner that ignores incompleteness and scales much better than a CPP planner, POND \citep{aij-mclug}.  Because the CPP planner scales so poorly (due to numerical underflow as the number of incomplete domain features increases to approximately twenty), detailed comparisons are somewhat uninformative; therefore, we locate discussion of the CPP planner in an appendix.    In the following, we provide background on the representation of the planning problems studied, a discussion of languages used to capture incomplete actions, a formulation of failure explanations, a definition of diagnosis and model counting, a planner based on failure propagation, a relaxed planning heuristic for failure propagation,  empirical evaluation, related work, and conclusion. 
%
\section{Background \& Representation}\label{sec:background}

Incomplete STRIPS minimally relaxes the classical STRIPS model to allow for possible preconditions and effects.  In the following, we review the STRIPS model and present incomplete STRIPS.

\und{STRIPS Domains} A STRIPS  \citep{strips} planning domain $D$  defines the tuple ($P$, $A$, $I$, $G$), where: $P$ is a set of propositions; $A$ is a set of action descriptions; $I \subseteq P$ defines a set of initially true propositions; and  $G \subseteq P$ defines the goal propositions.  Each action $a \in A$ defines $\text{pre}(a) \subseteq P$, a set of preconditions, $\text{add}(a) \subseteq P$, a set of add effects, and $\text{del}(a) \subseteq P$, a set of delete effects
%For example, consider the following domain, which we will use a running example:
%
%\begin{itemize}
%\item $P = \{p, q, r, g\}$
%\item $A = \{a, b, c\}$
%\begin{itemize}
%\item $\text{pre}(a) = \{p, q\}, \text{add}(a) = \{r\}, \text{del}(a) = \{\}$
%\item $\text{pre}(b) = \{p\}, \quad \text{add}(b) = \{r\}, \text{del}(b) = \{p\}$
%\item $\text{pre}(c) = \{q, r\}, \text{add}(c) = \{g\}, \text{del}(c) = \{\}$
%\end{itemize}
%\item $\text{add}(a_{-1}) = \{p, q\}$
%\item $\text{pre}(a_{n}) = \{g\}$
%\end{itemize}        
A plan $\pi = (a_0, ..., a_{n-1})$ in $D$ is a sequence of actions, which corresponds to a sequence of states $(s_0, ..., s_n)$, where $s_0 = I$, $\text{pre}(a_t) \subseteq s_t$ for $t = 0,..., n-1$, $G \subseteq s_n$, and
$s_{t+1} = s_t \backslash \text{del}(a_t) \cup \text{add}(a_t)$ for $t = 0,..., n-1$.
%We omit $a_{-1}$ and $a_n$ from the plans in our discussion when appropriate, with the understanding that each plan must use the initial and goal actions.
%
%For example, the plan $(a, b, c)$ corresponds to the state sequence $(s_0 = \{p, q\}, s_1 = \{p, q, r\}, s_2 = \{q, r\}, s_3 = \{q, r, g\})$, where the goal is satisfied in $s_3$.




\und{Incomplete STRIPS Domains}
Incomplete STRIPS domains are identical to STRIPS domains, with the exception
that the actions are incompletely specified.  Much like planning with incomplete
state information \citep{pff,aij-mclug}, the action incompleteness is not
completely unbounded.  The preconditions and effects of each action can be any
subset of the propositions $P$; the incompleteness is with regard to a lack of
knowledge about which of the subsets correspond to each precondition and effect.
 To narrow the possibilities, we find it convenient to refer to the {\em known},
{\em possible}, and {\em impossible} preconditions and effects.  For example, an
action's preconditions must consist of the known preconditions, and it must not
contain the impossible preconditions, but we do not know if it contains the
possible preconditions.  The union of the known, possible, and impossible
preconditions must equal $P$; therefore, an action can represent any two, and we
can infer the third.  We choose to represent the known and possible, but note
that GL represent the known and impossible; with the trade-off making our
representation more appropriate if there are fewer possible action features.


%In the following, we discuss incomplete domains and extend the complete domain model with features for possible preconditions and effects.  We note that an incomplete domain corresponds to a set of complete domains, each differing in terms of the inclusion of the possible features.


% \begin{definition}\label{def:incdomain}
An incomplete STRIPS domain $\tilde{D}$  defines the tuple ($P$, $\tilde{A}$,
$I$, $G$), where: $P$ is a  set of propositions; $\tilde{A}$ is a set of
incomplete action descriptions; $I \subseteq P$ defines a set of initially true
propositions; and $G \subseteq P$ defines the goal propositions. Each action
$\tilde{a} \in \tilde{A}$ defines $\text{pre}(\tilde{a}) \subseteq P$, a set of
known preconditions, $\widetilde{\text{pre}}(\tilde{a}) \subseteq P$, a set of
possible preconditions, $\text{add}(\tilde{a}) \subseteq P$, a set of known add
effects,  $\widetilde{\text{add}}(\tilde{a}) \subseteq P$, a set of possible add
effects, $\text{del}(\tilde{a}) \subseteq P$, a set of known delete effects, and
$\widetilde{\text{del}}(\tilde{a}) \subseteq P$, a set of possible delete
effects

Consider the following incomplete domain: $P = \{p, q, r, g\}$, $\tilde{A} = \{\tilde{a}, \tilde{b}, \tilde{c}\}$, $I = \{p, q\}$, and $G= \{g\}$.  The actions are defined: 
$\text{pre}(\tilde{a}) = \{p, q\}, \widetilde{\text{pre}}(\tilde{a})  = \{r\}, \widetilde{\text{add}}(\tilde{a}) = \{r\},  \widetilde{\text{del}}(\tilde{a}) = \{p\}$, $
  \text{pre}(\tilde{b}) = \{p\}, 
 \text{add}(\tilde{b}) = \{r\}, 
  \text{del}(\tilde{b}) = \{p\}, \widetilde{\text{del}}(\tilde{a}) = \{q\}$, and 
 $ \text{pre}(\tilde{c}) = \{r\}, \widetilde{\text{pre}}(\tilde{c})  = \{q\}, 
\text{add}(\tilde{c}) = \{g\}$.

The set of incomplete domain features ${\sf F}$ is comprised of the
following propositions for each $\tilde{a} \in \tilde{A}$:
 $\widetilde{\text{pre}}(\tilde{a}, p)$ if $p \in
 \widetilde{\text{pre}}(\tilde{a})$, $\widetilde{\text{add}}(\tilde{a}, p)$ if
 $p \in \widetilde{\text{add}}(\tilde{a})$, and
 $\widetilde{\text{del}}(\tilde{a}, p)$ if $p \in
 \widetilde{\text{del}}(\tilde{a})$.
% Each incomplete domain feature $f \in {\cal F}$ can result in a different type
% of plan fault (aligning with GL's original naming conventions):
% \begin{packed_itemize} \item Open precondition fault {\tt OP}($\tilde{a}, p$):
% if $\widetilde{\text{pre}}(\tilde{a}, p) \in {\sf F}$ and
% $\tilde{a}$ is applied to a state $s$ where $p$ is not true. \item Unlisted
% effect fault {\tt UE}($\tilde{a}, p$): if $\widetilde{\text{add}}(\tilde{a},
% p) \in {\sf F}$ and after $\tilde{a}$ is applied, $p$ is a
% precondition for another action. \item Possible clobberer fault {\tt
% PC}($\tilde{a}, p$): if $\widetilde{\text{del}}(\tilde{a}, p) \in {\cal
% F}(\tilde{D})$ and after $\tilde{a}$ is applied, $p$ is not reestablished by
% another action and $p$ is precondition. \end{packed_itemize} In this sense,
% each type of incomplete domain features can cause a plan fault if it can
% directly or indirectly prevent achievement of a subsequent action's
% precondition.  Each subset of ${\cal F}$ corresponds to an interpretation of
% the incomplete domain. \begin{definition}\label{def:interpretation}An
% interpretation $D^i$ of the incomplete domain $\tilde{D}$ is defined with
% respect to a subset of the incomplete domain features $F^i \subseteq {\cal F}$
% so that: \begin{itemize} \item $P^i = P$ \item $a_n^i = a_n$ \item $a_{-1}^i =
% a_{-1}$ \item For each $\tilde{a} \in \tilde{A}$ there exists an $a \in A^i$
% where \begin{itemize} \item $\text{pre}(a) = \text{pre}(\tilde{a}) \cup \{p |
% \text{pre}(\tilde{a}, p) \in F^i\}$ \item $\text{add}(a) =
% \text{add}(\tilde{a}) \cup \{p | \text{add}(\tilde{a}, p) \in F^i\}$ \item
% $\text{del}(a) = \text{del}(\tilde{a}) \cup \{p | \text{del}(\tilde{a}, p) \in
% F^i\}$ \end{itemize} \end{itemize} \end{definition}  We also refer to the set
% of incomplete features ${\cal F}(\tilde{a})$ that are specific to an action
% $\tilde{a}$.
An interpretation ${\sf F}^i \subseteq {\sf F}$ of the
incomplete STRIPS domain defines a STRIPS domain, in that every feature $f \in
{\sf F}^i$ indicates that a possible precondition or effect is a
respective known precondition or known effect; those features not in ${\cal
F}^i(\tilde{D})$ are impossible preconditions or effects.

A plan $\pi$ for $\tilde{D}$ is a sequence of actions, that when applied, {\em can lead} to a state where the goal is satisfied.  A plan $\pi = (\tilde{a}_0, ..., \tilde{a}_{n-1})$ in an incomplete domain $\tilde{D}$ is sequence of actions, that corresponds to the {\em optimistic} sequence of states $(s_0, ..., s_n)$, where $s_0 = I$, $\text{pre}(\tilde{a}_t) \subseteq s_t$ for $t = 0,..., n$, $G \subseteq s_n$, and $s_{t+1} = s_t \backslash \text{del}(\tilde{a}_t) \cup \text{add}(\tilde{a}_t) \cup \widetilde{\text{add}}(\tilde{a}_t)$ for $t = 0,..., n-1$.  

For example, the plan $(\tilde{a}, \tilde{b}, \tilde{c})$ corresponds to the state sequence $(s_0 = \{p, q\}, s_1 = \{p, q, r\}, s_2 = \{q, r\}, s_3 = \{q, r, g\})$, where the goal is satisfied in $s_3$.  

%so that ${\cal F}(\tilde{a}) = \{\text{pre}(\tilde{a}, p) | \text{pre}(\tilde{a}, p) \in {\sf F}\} \cup$ $\{\text{add}(\tilde{a}, p) | \text{add}(\tilde{a}, p) \in {\sf F}\} \cup$ $\{\text{del}(\tilde{a}, p) | \text{del}(\tilde{a}, p) \in {\sf F}\}$.
%
%For example, the complete domain example from the previous section is an interpretation of incomplete domain above, where $F^0 = \{\text{add}(\tilde{a}, r), \text{pre}(\tilde{c}, q)\}$.
%
%
%\begin{theorem}\label{thrm:interpretation}
%If $\pi = (\tilde{a}_{-1}, \tilde{a}_0, ..., \tilde{a}_{n-1}, \tilde{a}_{n})$ in a plan for  $\tilde{D}$, then there exists at least one interpretation $D^i$ in which  $\pi^i = (a^i_{-1}, a^i_0, ...,a^i_{n-1}, a^i_{n})$ is a plan.
%\end{theorem}
%\begin{proof}
%Define the interpretation $D^i$ so that $F^i = \{ \text{add}(\tilde{a}, p) |  \text{add}(\tilde{a}, p) \in {\sf F}\}$.  Then, for each action $a^i \in A^i$, we have $\text{add}(a^i) = \text{add}(\tilde{a}) \cup \widetilde{\text{add}}(\tilde{a})$.  Therefore, $s_{t+1} = s_t \backslash \text{del}(\tilde{a}_t) \cup \text{add}(\tilde{a}_t) \cup \widetilde{\text{add}}(\tilde{a}_t) = s_t \backslash \text{del}(a^i_t) \cup \text{add}(a^i_t)$ for $t = 0,..., n-1$.  And, by Definition \ref{def:interpretation}, $a_n^i = a_n$
%and $a_{-1}^i = a_{-1}$, so that the same states are reached by both plans.  If the preconditions of all actions in the interpretation and incomplete domain are identical, and all states reached are identical, then a plan in the incomplete domain corresponds to a plan in the interpretation.
%\end{proof}

%We also find it convenient to refer to sets of interpretations by a propositional sentence, where each model of the sentence corresponds to a set $F_i \subseteq {\cal F}$.  For example, the sentence $(f_0 \wedge f_1 \wedge f_2)$ defined over propositions $\{f_0, f_1, f_2, f_3\}$, has the models $\{f_0, f_1, f_2, \neg f_3\}$ and $\{f_0, f_1, f_2, f_3\}$, which correspond to the sets $\{f_0, f_1, f_2\}$ and $\{f_0, f_1, f_2, f_3\}$.

  


%\noindent The incomplete action $\tilde{b}$ corresponds to two complete actions $b_1$ and $b_2$ (one for each interpretation of its incomplete features):
%
%\begin{itemize}
%\item $\text{pre}(b_1) = \{p\}, \text{add}(b_1) = \{r\}, \text{del}(b_1) = \{p\}$
%\item $\text{pre}(b_2) = \{p\}, \text{add}(b_2) = \{r\}, \text{del}(b_2) = \{p, q\}$
%\end{itemize}
%

% Our definition of the plan semantics sets a loose requirement that plans with
% incomplete actions succeed under the most  {\em optimistic} interpretation
% ${\cal F}^0(\tilde{D}) = \{ \widetilde{\text{add}}(\tilde{a}, p) |
% \widetilde{\text{add}}(\tilde{a}, p) \in {\sf F}\}$ (i.e., possible
% preconditions need not be satisfied and the possible add effects (but not the
% possible delete effects) are assumed to occur when computing successor states). 
%  However, we would prefer that plans succeed under as many interpretations as
% possible; as we show, constructing plans that succeed for more interpretations
% decreases the number of instances where we must re-plan as we learn about the
% true domain.


\und{Discussion} Our definition of the plan semantics sets a loose requirement
that plans with incomplete actions succeed under the most  {\em optimistic} conditions:  possible
preconditions need not be satisfied and the possible add effects (but not the
possible delete effects) are assumed to occur when computing successor states.  
This notion of optimism is similar to that of GraphPlan \citep{graphplan} in
that  both assert every proposition that could be made true at a particular time
even if only a subset of the propositions can actually be made true.  In
GraphPlan, there {\em may} exist a plan to establish a proposition if the
proposition appears in the planning graph, whereas in our definitions there {\em
does} exist an interpretation of the incomplete domain that will establish a
proposition if it appears in a state \citep{USU-CS-TR-11-001}, and this interpretation {\em
may} correspond to the true domain. In GraphPlan, failing to assert a proposition that may be
established could eliminate plans, and in our case, failing to assert a proposition would prevent us from computing interpretations of the incomplete
domain that achieve the goal.

We ensure that the plan is valid for the
least constraining (most optimistic) interpretation of the incomplete domain. 
If the plan can achieve the goal in the most optimistic interpretation then it
may achieve the goal in others, but if the goal is not reachable in this
interpretation then it cannot be reached in any interpretation \citep{USU-CS-TR-11-001}.  As we
will show, we can efficiently determine the interpretations in which a plan is
invalid and use the number of such failed interpretations as a plan quality metric.


\begin{figure*}[t]\centering
\vspace*{-1cm}
\includegraphics[width=\linewidth]{WeberBryceICAPS11Fig1.eps}
\vspace*{-1cm}\caption{\label{fig:example} Labeled Plan}
\end{figure*}



\section{Planning in Incomplete Domains}

We present a forward state space planner called \FFRISKY{} that attempts to minimize the number of interpretations of the incomplete domain that can result in plan failure.  \FFRISKY{} generates states reached under the optimistic interpretation of the incomplete domain, but labels each state proposition with the interpretations where it will be impossible to achieve the proposition.  As such, the number of interpretations labeling the goals reached by a plan indicates the number of failed interpretations.  By counting interpretations (i.e., propositional model counting), we can determine the quality (robustness) of a plan.

%\und{Label Propagation}  
\FFRISKY{} labels propositions and actions with domain interpretations that will respectively fail to achieve the proposition or fail to achieve the preconditions of an action.  That is, labels indicate the cases where a proposition will be false (i.e., the plan fails to establish the proposition).  Labels $d(\cdot)$ are represented as  propositional sentences over ${\sf F}$ whose models correspond to domain interpretations.  

Initially, each proposition $p_0 \in s_0$ is labeled $d(p_0) = \perp$ to denote that there are no failed interpretations affecting the initial state, and each $p_0 \not\in s_0$ is labeled $d(p_0)=\top$.  For all $t \geq 0$, we define: 
 \begin{align}
\label{eqn:actlabel}d(\tilde{a}_t) &=  
d(\tilde{a}_{t-1}) \vee \hspace*{-.35cm}\bigvee\limits_{p \in \text{pre}(\tilde{a})}\hspace*{-.39cm} d(p_t) \vee\hspace*{-.35cm} \bigvee\limits_{p \in \widetilde{\text{pre}}(\tilde{a})} \hspace*{-.35cm}(d(p_t)\hspace*{-.09cm}\wedge\hspace*{-.09cm} \widetilde{\text{pre}}(\tilde{a}_t, p)  ) %& : \text{ t $\geq$ 1 }
%\end{array}\right.
\\
\label{eqn:proplabel}\hspace*{-.1cm}d(p_{t+1}) &= \left\{
\begin{array}{l@{\;}l@{\;}r}
d(p_t) \wedge d(\tilde{a}_t) & : p \in \text{add}(\tilde{a}_t) \\
d(p_t) \wedge  (d(\tilde{a}_t) \vee \\
\hspace*{1.25cm} \neg\widetilde{\text{add}}(\tilde{a}_t, p)) & : p \in \widetilde{\text{add}}(\tilde{a}_t)\\
\top & : p \in \text{del}(\tilde{a}_t)\\
d(p_t) \vee  \widetilde{\text{del}}(\tilde{a}_t, p) % \;\text{or}\; \\
 %d(p_t) \vee  {\tt PC}(\tilde{a}_t, p) 
 &: p \in \widetilde{\text{del}}(\tilde{a}_t)\\
d(p_t) & : \text{otherwise} 
\end{array}
\right. 
\end{align}
\noindent where $d(\tilde{a}_{-1}) = \perp$. The intuition behind the label propagation is that in Equation \ref{eqn:actlabel} an action will fail  in the domain interpretations $d(\tilde{a}_t)$ where a prior action failed, a known precondition is not satisfied, or a possible precondition (which is a known precondition for the interpretation) is not satisfied. As defined by Equation \ref{eqn:proplabel}, the plan will fail to achieve a proposition at time $t+1$ in all interpretations where i) the plan fails to achieve the proposition at time $t$ and the action fails, ii) the plan fails to achieve the proposition at time $t$ and the action fails or it does not add the proposition in the interpretation, iii) the action deletes the proposition, iv) the plan fails to achieve the proposition at time $t$ or in the interpretation the action deletes the proposition, or v) the action does not affect the proposition and any prior failed interpretations still apply.

A consequence of our definition of action failure is that each action fails if any prior action fails.  This definition follows from the semantics that the state becomes undefined if we apply an action whose preconditions are not satisfied.  While we use this notion in plan synthesis, we explore the semantics that the state does not change (i.e., it is defined) upon failure when we discuss acting in incomplete domains.  The reason that we define action failures in this manner is that we can determine all failed interpretations affecting a plan $d(\pi)$, defined  by  $d(\tilde{a}_{n-1}) \vee \bigvee_{g \in G} d(g_n)$.
%%Note that under the incomplete semantics, the rules never negate a single explanation.  The cost of using prime implicants to represent the explanations arises through distributing disjunction over conjunction and removing subsumed clauses.    
%
%Finally, to count the number of interpretations under which a plan fails, we count the models of,
%$d(\pi) = d_n(\tilde{a}_n)$,
%\noindent which expresses the interpretations where any of the actions did not have its preconditions satisfied or the goal was not satisfied.  Recall that we require valid plans to achieve the goal under the optimistic semantics, so we are guaranteed that if $\text{pre}(a_n) \subseteq s_n$, then the plan will succeed in at least one interpretation of the incomplete domain.  
It is possible to determine the interpretations that fail to successfully execute the plan up to and including time $t$ by computing $d(\tilde{a}_t)$.  
%We also note that as long as $n$ is the earliest time that the goal is achieved, we are guaranteed that $d(\tilde{a}_{t}) \models d(\pi)$.  

For example, consider the plan depicted in Figure \ref{fig:example}.  The propositions in each state and each action at each time are labeled by the propositional sentence below it. The edges in the figure connecting the propositions and actions denote what must be true to successfully execute an action or achieve a proposition.  The dashed edges indicate that action incompleteness affects the ability of an action or proposition to support a proposition.  For example, $\tilde{a}$ possibly deletes $p$, so the edge denoting its persistence is dashed.  The propositional sentences  $d(\cdot)$  below each proposition and action denote the domain interpretations where a action will fail or a proposition will not be achieved.  For example, $\tilde{b}$ at time one, ${\sf \tilde{b}_1}$, will fail if either $\text{pre}(\tilde{a}, r)$ or $\text{ del}(\tilde{a}, p)$ is true in the interpretation.  Thus, $d(\pi) = \widetilde{\text{pre}}(\tilde{a}, r) \vee \widetilde{\text{del}}(\tilde{a}, p)\vee
  (\widetilde{\text{del}}(\tilde{b}, q) \wedge \widetilde{\text{pre}}(\tilde{c}, q))$ and any domain interpretation satisfying $d(\pi)$ will fail to execute the plan and achieve the goal.




\section{Heuristics In Incomplete Domains}

Similar to propagating failed interpretation labels in a plan, we can propagate
labels in the relaxed planning problem to compute a search heuristic.  The
heuristic is the number of actions in a relaxed plan, and, while we do not use
the number of failed domain interpretations as the primary heuristic, we use the
failure labels to bias the selection of the relaxed plan actions and break ties
between search nodes with an equivalent number of actions in their relaxed
plans.  We solve the relaxed planning problem using a planning graph and 
start with a brief description of planning graphs.

\und{Planning Graph Heuristics}   A relaxed planning graph  is a layered graph of sets of vertices $({\cal P}_t, {\cal A}_t, ..., {\cal A}_{t+m}, {\cal P}_{t+m+1})$.  The planning graph built for a state $s_t$ defines ${\cal P}_t = \{p_t | p \in s_t\}$, ${\cal A}_{t+k} = \{ a_t | \forall_{ p \in \text{pre}(a)} p_t \in {\cal P}_{t+k}, a \in A \cup A(P)\}$, and ${\cal P}_{t+k+1} = \{p_{t+k+1} | a_{t+k} \in {\cal A}_{t+k}, p \in \text{add}(a)\}$, for $k = 0, ..., m$.  The set $A(P)$ includes noop actions for each proposition, such that $A(P) = \{a(p) | p \in P, \text{pre}(a(p)) =\text{add}(a(p))=p, \text{del}(a(p))=\emptyset\}$.
%A simple heuristic, $h^{max}$ \citep{bonet99planning} for the number of actions to achieve the goal $\text{pre}(a_{n})$ from $s_t$ is equivalent to the minimum level $k$ where the goal propositions are reached,  $h^{max} =\max_{g \in G} \min_{k: g_{t+k} \in {\cal P}_{t+k}} k$.  
The $h^{FF}$  heuristic \citep{hoffmann:nebel:jair-01} solves this relaxed planning problem by choosing actions from ${\cal A}_{t+m}$ to support the goals in ${\cal P}_{t+m+1}$, and recursively for each chosen action's preconditions, counting the number of chosen actions.

 
\und{Incomplete Domain Heuristics} Propagating failed interpretations in the
planning graph resembles propagating failed interpretations over a plan.  The
primary difference is how we define the failed interpretations for a proposition
when the proposition has multiple sources of support; recall that we allow only
serial plans and at each time each state proposition is supported by persistence
and/or a single action -- action choice is handled in the search space. In a
level of the relaxed planning graph, there are potentially many actions
supporting a proposition, and {\em we select the supporter with the fewest
failed interpretations}. The chosen supporting action, denoted
$\hat{a}_{t+k}(p)$, determines the failed interpretations affecting a proposition $p$ at level
$t+k+1$.


A relaxed planning graph with propagated labels is a layered
graph of sets of vertices of the form $(\hat{\cal P}_t, \hat{\cal A}_t, ..., \hat{\cal
A}_{t+m}, \hat{\cal P}_{t+m+1})$. The relaxed planning graph built for a state $\tilde{s}_t$ defines $\hat{\cal P}_0 = \{\hat{p}_t | p \in \tilde{s}_t\}$, $\hat{\cal A}_{t+k} = \{ \hat{a}_{t+k} | \forall_{p \in \text{pre}(\tilde{a})} \hat{p}_{t+k} \in \hat{\cal P}_{t+k}, \tilde{a} \in \tilde{A} \cup A(P)\}$, and $\hat{\cal P}_{t+k+1} = \{p_{t+k+1} | \hat{a}_{t+k} \in \hat{\cal A}_{t+k}, p \in \text{add}(\tilde{a}) \cup \widetilde{\text{add}}(\tilde{a})\}$, for $k = 0, ..., m$.  Much like the successor function used to compute next states, the relaxed planning graph assumes an optimistic semantics for action effects by adding possible add effects to proposition layers, but, as we will explain below, it associates failed interpretations with the possible adds. 

 Each planning graph vertex has a label, denoted $\hat{d}(\cdot)$.  The failed interpretations $\hat{d}(p_t) $ affecting a proposition are defined such that $\hat{d}(p_t) = d(p_t)$, and for $k \geq 0$, 
\begin{align}
\widehat{d}(\tilde{a}_{t+k}) &= \hspace*{-.2cm} 
\bigvee\limits_{p \in \text{pre}(\tilde{a}) } \hspace*{-.2cm}   \hat{d}(p_{t+k}) \vee \hspace*{-.25cm} 
\bigvee\limits_{p \in \widetilde{\text{pre}}(\tilde{a})} \hspace*{-.25cm}  (\hat{d}(p_{t+k})  \wedge  \widetilde{\text{pre}}(\tilde{a}, p) )\label{eqn:hact}\\
 \hspace*{-.6cm} \hat{d}(p_{t+k+1}) &= 
\left\{\begin{array}{l@{\;}l@{\;}r}
\widehat{d}(\hat{a}_{t+k}(p)) & : p \in \text{add}(\hat{a}_{t+k}(p))\\
\widehat{d}(\hat{a}_{t+k}(p)) \vee \\
\neg\widetilde{\text{add}}(\hat{a}_{t+k}(p), p) & : p \in \widetilde{\text{add}}(\hat{a}_{t+k}(p))\end{array}\right. \hspace*{-.1cm}\label{eqn:hprop}
\end{align}
\noindent 
%Propositions in the planning graph initially have the same faults associated with them as in state $\tilde{s}_t$ and are defined by $d(\cdot)$.  
Every action in every level $k$ of the planning graph will fail in any interpretation where their preconditions are not supported (Equation \ref{eqn:hact}).  A proposition will fail to be achieved in any interpretation where the chosen supporting action fails to add the proposition (Equation \ref{eqn:hprop}).

We note that the rules for propagating labels in the planning graph differ from the rules for propagating labels in the state space.  In the state space, the action failure labels include interpretations where any prior action fails.  In the relaxed planning problem, the action failure labels include only interpretations affecting the action's preconditions, and not prior actions; it is not clear which actions will be executed prior to achieving a proposition because many actions may be used to achieve other propositions at the same time step.  

%\und{Choosing Supporters} While only a single action or noop may be required to support a proposition, using multiple supporters can increase the number of incomplete domain interpretations that support it (by ensuring that not all sources of support are subject to the same faults).  We wish to select a set $\hat{\cal S}_{t+k}(p) \subseteq \{\tilde{a} \in \hat{\cal A}_{t+k} | p \in \text{add}(\tilde{a}) \cup \widetilde{\text{add}}(\tilde{a})\}$ to define a most preferred $\hat{d}_{t+k+1}(p)$ (i.e., have fewer explanations of failure).  We use a greedy algorithm to incrementally add actions to $\hat{\cal S}_{t+k}(p)$, and check if $\hat{d}_{t+k+1}(p)$ is improved.  
%
%The greedy algorithm proceeds as follows.  We consider all singleton sets of actions, and select the most preferred (having the fewest failure explanation models or prime implicants, and breaking ties by selecting noop actions).  To the most preferred action set, we add each action and determine the most preferred, two element action set.  If none of the two element action sets are more preferred than the best single action set, we define $\hat{\cal S}_{t+k}(p)$ as the best single action set.  Otherwise, the algorithm continues to extend the best action set with one action at a time until it cannot find a more preferred action set (by counting models or prime implicants).
%
% alternative definitions (one for each possible supporter) of the set $\hat{\cal S}_{t+k}(p)$, and select the supporter that has the most preferred definition of  $\hat{d}_{t+k+1}(p)$ (breaking ties by first preferring noop actions and second preferring actions whose first appearance in the planning graph is earliest).  With a single element in $\hat{\cal S}_{t+k}(p)$, we evaluate all single element extensions of the set (making it a two element set), choosing the extension that most improves the measure of $\hat{d}_{t+k+1}(p)$ (depending on the chosen measure, either reducing the number of models, or prime implicants); if no extension improves the measure, then the algorithm terminates and returns $\hat{\cal S}_{t+k}(p)$ with a single element.  In our implementation, we allow at most two supporters per proposition, but it is possible to allow an arbitrary number of supporters in this fashion by extending the set of supporters greedily until no new extension improves its measure.
%



% \und{Heuristic Computation}   We terminate the relaxed planning graph expansion
% at the level $t+k+1$ where one of the following conditions is met: i) the
% planning graph reaches a fixed point where the labels do not change,
% $\hat{d}(p_{t+k}) = \hat{d}(p_{t+k+1})$ for all $p\in P$, or ii) the goals have
% been reached at $t+k+1$ and the fixed point has not yet been reached. Our
% $h^{\sim FF}$ heuristic makes use of the chosen supporting action
% $\hat{a}_{t+k}(p)$ for each proposition that requires support in the relaxed
% plan, and, hence, measures the number of actions used while attempting to
% minimize failed interpretations.  The other heuristic $h^{\sim M}$ measures the
% number of interpretations that fail to reach the goals in the last level (i.e.,
% such that  $h^{\sim M} = |M(\vee_{g \in G} \hat{d}(g_{t+m+1}))|$, where $m+1$ is
% the last level of the planning graph and $M(\psi)$ is the set of models of a
% propositional sentence $\psi$.  \FFRISKY{} uses both heuristics, treating 
% $h^{\sim FF}$ as the primary heuristic and using $h^{\sim M}$ to break ties.

\und{Heuristic Computation}   We terminate the relaxed planning graph expansion
at the level $t+k+1$ where one of the following conditions is met: i) the
planning graph reaches a fixed point where the explanations do not change,
${d}(p_{t+k}) = {d}(p_{t+k+1})$ for all $p\in P$, or ii) the goals have been
reached at $t+k+1$ and the fixed point has not yet been reached. Our $h^{\sim
FF}$ heuristic makes use of the chosen supporting action ${a}_{t+k}(p)$ for each
proposition that requires support in the relaxed plan, and, hence, measures the
number of actions used while attempting to minimize failed interpretations
(the supporting actions are chosen by comparing failure explanations). The other
heuristic $h^{\sim M}$ measures the number of interpretations that fail to reach the goals in the last level (i.e., such that  $h^{\sim M} = |M(\vee_{p \in
G} {d}(p_{t+m+1}))|$, where $m+1$ is the last level of the planning graph. 
\FFRISKY{} uses both heuristics, treating  $h^{\sim FF}$ as the primary
heuristic and using $h^{\sim M}$ to break ties.  While it is likely that
swapping the role of the heuristics may lead to better quality plans (fewer
failed interpretations), our informal experiments determined that the
scalability of \FFRISKY{} is greatly limited in such cases; measuring failed
interpretations is not correlated with solution depth in the search graph unlike
relaxed plan length.  The relaxed plans are informed by the propagated
explanations because we use the model count or prime implicant count to bias
action selection.

\section{Counting Models and Prime Implicants }

Failure explanations $d(\cdot)$ and $\hat{d}(\cdot)$ are propositional sentences that help bias decisions in search and heuristics.  Namely, we assume that we can count the number of propositional models of these sentences to indicate how many interpretations of the incomplete domain will fail to successfully execute a plan.  Model counting is intractable \citep{Roth96}, but by representing the sentences as OBDDs \citep{bryant-ieeetc86}, model counting is polynomial in the size of the OBDD \citep{darwiche} (which can be exponential sized in the worst case).  

In addition to OBDDs and model counting, we also explore counting prime implicants (PIs) -- also called diagnoses.  A set of PIs is a set of conjunctive clauses (similar to a DNF) where no clause is subsumed by another, and are used in model-based diagnosis to represent diagnoses (sets of incomplete features that must interact to cause system failure) \citep{dekleer}.  We find it useful to bound the cardinality (the number of conjuncts) of the PIs, effectively over-approximating the models of a propositional sentence.  

Instead of counting the models of two labels $d(\cdot)$ and $d'(\cdot)$, we can compare the number of PIs.  Our intuition is that having fewer diagnoses of failure is preferred, just as is having fewer models of failure (even though having fewer PIs does not always imply fewer models).  The advantage is that counting PIs is much less expensive than counting models, especially if we bound the cardinality of the PIs.  Finally, we use a heuristic when counting PIs whereby we compare two sets in terms of the number of cardinality-one PIs, and if equal, the number of cardinality-two PIs, and so on.  The intuition behind comparing PIs in this fashion is that smaller PIs are typically satisfied by a larger number of models and are thus more representative.  That is, a sentence with one cardinality-one PI will have more models than a sentence with one cardinality-two PI.

\section{Acting in Incomplete Domains} Acting in incomplete domains provides an opportunity to learn about the domain by observing the states resulting from action application.  In the following, we describe what our agent \goalie{} can learn from acting in incomplete domains and how it might achieve its goals.  
%We explore two strategies for plan execution that either  optimistically or pessimistically assess knowledge of the incomplete domain.  Namely, these agent strategies differ in how they will determine that a plan has failed, necessitating re-planning.  
\goalie{} will continue to execute a plan until it is faced with an action that is guaranteed to fail or it has determined that the plan failed in hindsight.
%, and the pessimistic agent executes a plan until either the current action may fail or a future action is guaranteed to fail.


%: 
%\begin{packed_itemize}
%\item {\bf Execute:} risk executing an action whose preconditions may be unsatisfied
%\item {\bf re-plan:} select a time to abandon a plan and re-plan
%\end{packed_itemize}

\goalie{} maintains a propositional sentence $\phi$ defined over ${sf
F} \cup \{fail\}$ which describes the current knowledge of the
incomplete domain.  The proposition $fail$ denotes whether \goalie{} believes
that its current plan failed -- it is not always possible to determine if an
action applied in the past did not have its preconditions satisfied.  Initially,
\goalie{} believes $\phi = \top$, denoting its complete lack of knowledge of the
incomplete domain and whether its current plan will fail.    If \goalie{}
executes an action $a$ in state $s$ and transitions to state $s'$, then it can
update its knowledge as $\phi \wedge o(s, a, s')$, where
\noindent \begin{eqnarray}
o(s, a, s') &=& \left\{ \begin{array}{ll}
(fail \wedge o^-) \vee  o^+  &: s = s'\\
o^+  & : s \not= s'
\end{array}\right. \label{eqn:update}\\
o^- &=& \bigvee\limits_{\substack{\widetilde{\text{pre}}(\tilde{a},p) \in {\sf F}:\\ p \not\in s} } \widetilde{\text{pre}}(\tilde{a},p) \label{eqn:update1} \\
o^+ &=& o^{pre} \wedge o^{add} \wedge o^{del}\label{eqn:update2}\\
o^{pre} &=& \bigwedge\limits_{\substack{\widetilde{\text{pre}}(\tilde{a},p) \in {\sf F}:\\ p \not\in s} }\hspace*{-0.5cm}\neg \widetilde{\text{pre}}(\tilde{a},p)\label{eqn:update3}  \\
o^{add} &=&  \hspace*{-.5cm}\bigwedge\limits_{\substack{\widetilde{\text{add}}(\tilde{a},p) \in {\sf F}:\\ p\in s'\backslash s} }\hspace*{-0.75cm}\widetilde{\text{add}}(\tilde{a},p)   \wedge  \hspace*{-0.5cm}\bigwedge\limits_{\substack{\widetilde{\text{add}}(\tilde{a},p) \in {\sf F}: \\ p \not\in  s\cup s'}} \hspace*{-0.75cm}\neg \widetilde{\text{add}}(\tilde{a},p)   \label{eqn:update4}\\
o^{del} &=& \hspace*{-.5cm} \bigwedge\limits_{\substack{\widetilde{\text{del}}(\tilde{a},p) \in {\sf F}: \\p \in s \backslash s'}} \hspace*{-0.75cm}\widetilde{\text{del}}(\tilde{a},p)  \wedge  \hspace*{-0.5cm}\bigwedge\limits_{\substack{\widetilde{\text{del}}(\tilde{a},p) \in {\sf F}:\\ p \in s\cap s'}} \hspace*{-0.75cm}\neg \widetilde{\text{del}}(\tilde{a},p)  \label{eqn:update5}
\end{eqnarray}
\noindent We assume that the state will remain unchanged when \goalie{} executes
an action whose precondition is not satisfied by the state, and because the
state is observable, Equation \ref{eqn:update} references the case where the
state does not change and the case where it changes.  If the state does not
change, then either the action failed and one of its unsatisfied possible
preconditions is a precondition (Equation \ref{eqn:update1}) or the action
succeeded (Equation \ref{eqn:update2}).  If the state changes, then \goalie{}
knows that the action succeeded.  If an action succeeds, \goalie{} can conclude
that i) each possible precondition that was not satisfied is not a precondition
(Equation \ref{eqn:update3}), ii) each possible add effect that appears in the
successor but not the predecessor state is an add effect and each that does not
appear in either state is not an add effect, iii) each possible delete effect
that appears in the predecessor but not the successor is a delete effect and
each that  appears in both states is not a delete effect.

Using $\phi$, it is possible to determine if the next action in a plan, or any
subsequent action, can or will fail.  If  $\phi \wedge d(a_{t+k})$ is
satisfiable, then $a_{t+k}$ {\em can} fail, and if $\phi \models d(a_{t+k})$,
then $a_{t+k}$ {\em will}  fail.  \goalie{} will execute an action if it may not
fail, even if later actions in its plan will fail.  If \goalie{} determines that
its next action will fail, or a prior action failed ($\phi \models fail$), then
it will re-plan.  \goalie{}  uses $\phi$ to modify the actions during
re-planning by checking for each incomplete domain feature $f \in {\sf
F}$ if $\phi \models f$ or if $\phi \models \neg f$.  Each such
literal entailed by $\phi$ indicates if the respective action has the possible
feature as a known or impossible feature; all other features remain as possible
features.


%In addition to our two agent types, we investigate two types of environments, differing in their semantics for failed action application.  The first, which is consistent with the assumptions of \FFRISKY{}, assumes that applying an action when its preconditions are not satisfied will lead to an undefined state where no further actions can be applied.  The second assumes that applying an action with unsatisfied preconditions will result in no change to the state and further actions are allowed.  In both cases, a failed action application will result in no change of state reported by the environment, but the former will report no change for all actions, and the latter will report changes for subsequent actions whose preconditions are satisfied.  The purpose of using two types of environments is to illustrate the relative benefits of the pessimistic and optimistic agents.

\begin{algorithm}[t]
\SetLine
\KwIn{state $s$, goal $G$, actions $\tilde{A}$}

%\begin{algorithm}
 $\phi \leftarrow \top$; $\pi \leftarrow Plan(s, G, \tilde{A}, \phi)$\;
\While{$\pi \not= ()$ and $G\not\subseteq s$}{
 $a \leftarrow \pi.first()$;
 $\pi \leftarrow \pi.rest()$\;
\eIf{$\text{pre}(a) \subseteq s$ and $\phi \not\models \bigvee\limits_{\substack{\widetilde{\text{pre}}(\tilde{a},p) \in {\sf F}: p \not\in s} } \widetilde{\text{pre}}(\tilde{a},p)
$}{
%$\phi \not\models d(a)$\COMMENT{Action may succeed}} 
	$s ' \leftarrow Execute(a)$\;
	 $\phi \leftarrow \phi \wedge o(s, a, s')$\;
	 $s \leftarrow s'$\;
}
{
	 $\phi \leftarrow \phi \wedge fail$\;
}

\If{$\phi \models fail$ }{
	 $\phi \leftarrow \exists_{fail}  \phi$\;
	 $\pi \leftarrow Plan(s, G, \tilde{A}, \phi)$\;
}
}
\caption{\goalie{}$(s, G, \tilde{A})$}\label{alg:replan}
\end{algorithm}

Algorithm \ref{alg:replan} is the strategy used by \goalie{}.  The algorithm
involves initializing the agent's knowledge and plan (line 1), and then while
the plan is non-empty and the goal is not achieved (line 2) the agent proceeds
as follows.  The agent selects the next action in the plan (line 3) and
determines if it can apply the action (line 4).  If it applies the action, then
the next state is returned by the environment/simulator (line 5) and the agent
updates its knowledge (line 6 and Equation \ref{eqn:update}) and state (line 7),
otherwise the agent determines that the plan will fail (line 9).  If the plan
has failed (line 11), then the agent forgets its knowledge of the plan failure
(line 12) and finds a new plan using its new knowledge (line 13). \goalie{} is
not guaranteed success, unless it can find a plan that will not fail (i.e., 
$d(\pi) = \perp$).

\goalie{} is not hesitant to apply actions that may fail because trying actions
is its only way to learn about them.  However, \goalie{} is able to determine
when actions will fail and re-plans.  More conservative strategies are possible
if we assume that \goalie{} can query a knowledge engineer about action features
to avoid potential plan failure, but we leave such goal-directed knowledge
acquisition for future work.


\section{Empirical Evaluation}\label{sec:empirical}
%
The empirical evaluation is divided into four sections:  the domains used for the experiments, the test setup used, results for off-line planning, and results for on-line planning and execution.  The questions that we would like to answer include: 
\begin{packed_itemize}
\item Q1: Does reasoning about incompleteness lead to high quality plans?
%\item Can an existing CPP planner address planning in incomplete domains?
%\item Can a classical planner (that ignores action incompleteness) find reasonable quality solutions in incomplete domains?
%\item How well does a planner that counts failure explanation models scale?
\item Q2: Does counting prime implicants perform better than counting models?
\item Q3: As the number of incomplete features grows, does stronger reasoning
about incompleteness help?
\item Q4: Does reasoning about incompleteness save overall time in planning and execution?
%\item Q4: Can a pessimistic agent achieve their goals as often as an optimistic agent?
\end{packed_itemize}





\und{Domains} We use four domains in the evaluation: a modified Pathways,
Bridges,  a modified PARC Printer, and Barter World.  In
all domains, we derived multiple instances by randomly (with probabilities 0.25,
0.5, 0.75, and 1.0 for each action) injecting incomplete  features.   
With these variations of the domains, the instances include up to ten thousand
incomplete  features each. All planning results are taken from ten random
instances (varying $\sf F$) of each problem and within these, each planning
and execution result is one of ten ground-truth domains selected by the
simulator.
% The problem instances and generators are available at \href{}{\it withheld for
% blind review}.

The Pathways (PW) domain from the International Planning Competition  (IPC) involves actions that model chemical reactions in signal
transduction pathways.  Pathways is a naturally incomplete domain where the lack
of knowledge of the reactions is quite common because they are an active
research topic in biology.  

The Bridges (BR) domain consists of a traversable grid and the task is to find a
different treasure at each corner of the grid. In Bridges,
a bridge might be required  to cross between some grid locations (a possible
precondition), many of the bridges may have a troll living
underneath that will take all the treasure accumulated (a possible delete
effect), and the corners may give additional treasures (possible add
effects).  Grids are square and vary in dimension (2-16).

The PARC Printer (PP) domain from the IPC involves planning paths for sheets of
paper through a modular printer.  A source of domain incompleteness is that a module
accepts only certain paper sizes, but its documentation is incomplete.  Thus,
paper size becomes a possible precondition to actions using the module.

The Barter World (BW) domain involves navigating a grid and bartering items to
travel between locations.  The domain is incomplete because actions that acquire
 items are not always known to be successful (possible add effects) and traveling between locations may require
certain items (possible preconditions) and may result in the loss of an item
(possible delete effects). Grids vary in dimension (2-16) and items
in number (1-4).




\begin{table}\centering \begin{tabular}{|l|rrrrr|}      \hline &  FF  & 
PI1 & PI2  &  PI3  &  BDD  \\\hline
FF	&		0		&		155		&		161		&		161		&		123		\\
PI1	&	{\bf	629}	&		0		&	{\bf	79}	&	{\bf	78}	&	{\bf	208}	\\
PI2	&	{\bf	619}	&		77		&		0		&		46		&	{\bf	208}	\\
PI3	&	{\bf	594}	&		62		&	{\bf	51}	&		0		&	{\bf	199}	\\
BDD & {\bf 512} &  189  &  189  &  187  &  0  \\\hline
\end{tabular}																						
\caption{\label{tab:qualcomp} Num Instances with Better Quality.}
\end{table}

\begin{figure}\centering
\includegraphics[width=\linewidth]{WeberBryceICAPS11Fig2.eps}
\caption{\label{fig:alltotaltime}Cumulative Time  in  All Domains.}
\end{figure} 
   

\und{Test Setup} The tests were run on a Linux machine with a 3 Ghz Xeon
processor, a memory limit of 2GB, and a time limit of 20 minutes per run for the
off-line planning invocation and 60 minutes for each on-line planning and
execution invocation.  All code was written in Java and run on the 1.6 JVM. 
\FFRISKY{} uses a greedy best first search with deferred heuristic evaluation
and a dual-queue for preferred and non-preferred operators
\citep{DBLP:journals/jair/Helmert06}.  
% Both planners also use the same planning
% graph implementation.
% POND was run using ten planning graph samples per search node heuristic
% evaluation in a weighted (w=5) A* search, and  $\overline{\tau}$ equal to the
% minimum proportion of incomplete action instances that any other method
% satisfies the goal for the same problem.

%Those planners that solve more problems can be easily identified, and their overall relative plan quality and efficiency are evident by the cumulative plots.

%\und{\FFRISKY{} Implementation}The \FFRISKY{} planner is implemented in Java, and each of configurations of the planner share common source code, with the exception of their respective techniques for fault propagation in the state space and heuristic computation.  

We use five configurations of the planner: \FFRISKY{}-$FF$, \FFRISKY{}-$PIk$ (k
= 1, 2, 3), and \FFRISKY{}-$BDD$, that differ in how they reason about domain
incompleteness.  \FFRISKY{}-$FF$ does not compute failure explanations and uses
the FF heuristic;  it is inspired by the planner used by CA because it is likely
to find a plan that will work for only the most optimistic domain
interpretation.  \FFRISKY{}-$PIk$, where $k$ is the bound on the cardinality of
the prime implicants, uses only prime implicants to compare failure
explanations.  \FFRISKY{}-$BDD$ uses OBDDs to represent and count failure
explanations.  The number of failed interpretations for a plan $\pi$ found by
any of the planners is reported herein by counting models of an OBDD
representing $d(\pi)$. The versions of the planner are compared by the
proportion of interpretations of the incomplete domain that achieve the goal and
total planning time in seconds. The plots in the following section depict these
results using the cumulative percentage of successful domain interpretations and
planning time to identify the performance over all problems and domains.  We
also report detailed results on the number of solved problems per domain.

We also compare the off-line planning results to a conformant probabilistic
planner POND \citep{aij-mclug} that solves translated instances of the incomplete
STRIPS problems.  We set the minimum required probability of goal satisfaction
to the minimum proportion of successful domain interpretations of plans found by
the other approaches. We do not provide the details of the translation because
the results are very poor, but refer the reader to an extended verison of this work \citep{USU-CS-TR-11-001}.  We attempted a comparison to PFF \citep{pff}, but the implementation proved unstable for all but the smallest instances. 


\begin{table}[t]				\centering																				
\begin{tabular}{|l|r|r@{ }r@{ }r@{ }r|r|}																								
\hline																								
Domain	&		FF		&		PI1		&		PI2		&		PI3		&		BDD		&	POND	\\ \hline	\hline
PP 0.25	&		130		&		83		&		85		&	{\bf	86}	&		80		&	10	\\	
PP 0.5	&		130		&		87		&	{\bf	88}	&		87		&		80		&	0	\\	
PP 0.75	&		130		&		82		&	{\bf	83}	&		81		&		80		&	0	\\	
PP 1.0	&		13		&	{\bf	10}	&		9		&		9		&		8		&	0	\\	\hline
PP	&		403		&		262		&	{\bf	265}	&		263		&		248		&	10	\\	\hline\hline
BR1 0.25 	&		40		&	{\bf	22}	&	{\bf	22}	&	{\bf	22}	&	{\bf	22}	&	2	\\	
BR1 0.5 	&		39		&	{\bf	20}	&	{\bf	20}	&	{\bf	20}	&	{\bf	20}	&	2	\\	
BR1 0.75 	&		36		&	{\bf	19}	&	{\bf	19}	&	{\bf	19}	&	{\bf	19}	&	2	\\	
BR1 1.0 	&		4		&	{\bf	2}	&	{\bf	2}	&	{\bf	2}	&	{\bf	2}	&	1	\\	
BR2 0.25 	&		38		&		20		&		20		&		20		&	{\bf	21}	&	3	\\	
BR2 0.5 	&		35		&	{\bf	25}	&	{\bf	25}	&	{\bf	25}	&		23		&	3	\\	
BR2 0.75 	&		35		&	{\bf	22}	&		21		&		21		&		21		&	2	\\	
BR2 1.0 	&		4		&	{\bf	2}	&	{\bf	2}	&	{\bf	2}	&	{\bf	2}	&	1	\\	
BR3 0.25 	&		45		&	{\bf	36}	&	{\bf	36}	&	{\bf	36}	&	{\bf	36}	&	1	\\	
BR3 0.5 	&		47		&	{\bf	33}	&	{\bf	33}	&	{\bf	33}	&		32		&	2	\\	
BR3 0.75 	&		46		&		39		&		39		&		39		&	{\bf	41}	&	1	\\	
BR3 1.0 	&		5		&	{\bf	4}	&	{\bf	4}	&	{\bf	4}	&		3		&	1	\\	\hline
BR 	&		374		&	{\bf	244}	&		243		&		243		&		242		&	21	\\	\hline\hline
BW 0.25 	&		150		&		106		&		128		&	{\bf	129}	&		108		&	60	\\	
BW 0.5 	&		150		&		134		&	{\bf	137}	&		134		&		118		&	45	\\	
BW 0.75 	&		150		&	{\bf	140}	&		138		&		137		&		111		&	27	\\	
BW 1.0 	&		15		&	{\bf	14}	&	{\bf	14}	&	{\bf	14}	&		11		&	2	\\	\hline
BW 	&		465		&		394		&	{\bf	417}	&		414		&		348		&	155	\\	\hline\hline
PW 0.25 	&		160		&	{\bf	40}	&	{\bf	40}	&	{\bf	40}	&	{\bf	40}	&	19	\\	
PW 0.5 	&		160		&	{\bf	70}	&		60		&		50		&		60		&	13	\\	
PW 0.75 	&		170		&	{\bf	60}	&		50		&		40		&	{\bf	60}	&	12	\\	
PW 1.0 	&		19		&		5		&		6		&		6		&	{\bf	7}	&	2	\\	\hline
PW 	&		509		&	{\bf	175}	&		156		&		136		&		167		&	46	\\	\hline\hline
Total	&		1751		&		1075		&	{\bf	1081}	&		1056		&		1005		&	232	\\	
\hline																								
\end{tabular}	\caption{\label{tab:solved} Instances Solved By Domain}																							
\end{table}																																												



\und{Off-line Planning Results} Figure \ref{fig:alltotaltime} plots the
cumulative cumulative total planning time.  To enhance readability, every one
hundredth data point is plotted in the figures (while still representative of the true cumulative
number).  Table \ref{tab:qualcomp} lists the number of times that the
configuration in the row finds a better solution than the configuration in the
column when both solve a problem, with the best of each pair bolded. Table
\ref{tab:solved} lists the number of solved problems for each planner and
highlights the most solved for each domain in bold. 

We see that Q1 is answered positively by the results.  Plan quality is
improved by reasoning about incompleteness (through \FFRISKY{}-$PIk$ or -$BDD$),
but scalability suffers.  However, we note that minimizing the number of failed
interpretations  can be phrased as a conformant probabilistic
planning problem, which is notoriously difficult \citep{pff,aij-mclug}, and
expecting the scalability of a classical planner is  unreasonable.
% Of the problems solved by our $PIk$ or $BDD$ approaches, the total time can be
% less than the $FF$ approach in some cases; while we have not characterized the
% exact reason for this performance, it is likely to stem from positive
% interactions among actions selected when reasoning about incompleteness that
% was otherwise ignored by the $FF$ approach.

Q2 is answered overall positively by our experiments because the $PIk$
approaches solve more problems with better quality and in less time than the
$BDD$ approach.  

Q3 is answered negatively because as the probability of injecting incomplete
features grows from 0.25 to 1.0 the $PI3$ approach initially solves the most
problems, but then $PI2$, and then $PI1$ solve the most problems in each domain.
A possible explanation for this result is that it becomes too costly to reason
about incompleteness as it increases and that a more coarse approach is needed;
however, the $BDD$ approach while not the best, seems to degrade less as the
incompleteness increases.  It is likely that the OBDD package (JDD)
implementation is to credit for the $BDD$ approach's performance because model
counting can become prohibitively expensive in larger problems.

Table \ref{tab:solved} indicates that POND is not competitive and suggests that
existing approaches are not directly applicable to planning in incomplete
domains.  We note that \FFRISKY{} is inspired by POND, but employs more
approximate reasoning about incompleteness by using bounded prime implicants
(see \citep{USU-CS-TR-11-001} for a more thorough discussion).

% However, we note that $PI1$ performs best in the Barter World
% domain, which was specifically designed to have failure explanations with high
% cardinality prime implicants, which also has an impact upon OBDD-based
% representations.  In this domain, we are faced with a high cost for model
% counting that becomes prohibitive.  Thus, counting bounded prime implicants is a
% viable option when model counting is too costly.

% While i t is not clear whether approximating the OBDD representation and using
% model counting, rather than using and counting bounded prime implicants, will
% lead to better results, we note that the $PIk$ approaches are fast but may not
% give enough guidance to the planner and the $BDD$ approach is slower but more
% robust.
       
% \begin{figure}[t]\vspace*{-.5cm}
% \centering\includegraphics[width=.9\linewidth]{./alldom-deadline.eps}\\
% \includegraphics[width=.9\linewidth]{./alldom-deadline-time.eps}
% \caption{\label{fig:plan} Cumulative Plan Success and Planning Time}
% \end{figure}
 
  
\begin{figure*}[t]    
\hspace*{-1cm}
\includegraphics[width=.42\linewidth]{./WeberBryceICAPS11Fig3a.eps}\hspace*{-1.5cm}
\includegraphics[width=.42\linewidth]{./WeberBryceICAPS11Fig3b.eps}\hspace*{-1.5cm}
\includegraphics[width=.42\linewidth]{./WeberBryceICAPS11Fig3c.eps} 
%\hspace*{-2.25cm}       
%\includegraphics[width=.4\linewidth]{./hobonavLearning.eps}
\caption{\label{fig:exec} Comparison of Goalie with \FFRISKY-$FF$ and
\FFRISKY-$PI1$.}
\end{figure*}		   

 

\und{On-line Planning and Execution Results} Figure \ref{fig:exec} depicts a
comparison between Goalie using \FFRISKY{}-$FF$ and \FFRISKY{}-$BDD$ to
synthesize plans, so that we can judge whether planning and execution strategies
such as that of CA will benefit from planners reason about incompleteness.  The
scatter plots in the figure show the respective number of actions applied to
achieve the goal, the number of plans generated, and the total planning and execution time.  

Q3 is answered mostly negatively. 
The plot of the total time taken shows that the planners are somewhat mixed or even for
times less than 10 seconds.  However, for times greater than 10 seconds, it
appears that using \FFRISKY-$FF$ in \goalie{} can take up to an order of
magnitude less time.  However, there are several difficult instances in
which \FFRISKY-$PI1$ does outperform \FFRISKY-$FF$.  By investigating the plots
of the number of actions taken and the number of plans generated, it is apparent
that \FFRISKY-$PI1$ takes fewer actions as the instances require near 100 steps,
and tends to re-plan less often.  We expect that more efficient implementations
of reasoning about prime implicants (e.g., tries) could lower the cost of
planning with \FFRISKY{}, making it able to capitalize on its better quality
plans.


\section{Related Work}

Planning in incomplete domains is noticeably similar to planning with incomplete
information, where action descriptions instead of states are incomplete. 
Incomplete domains can be translated to conformant probabilistic planning
domains, and planners such as POND \citep{aij-mclug} and PFF \citep{pff} are
applicable.  However, while the translation is theoretically feasible, practical
issues regarding numeric precision prohibit effective use of existing planners. 
While we do not report results, we translated the instances described here and
ran both POND and PFF; both planners compared equally with \FFRISKY{} on the
problems where numeric precision was not exceeded.

Our investigation is an instantiation of model-lite planning \citep{modellite}.  Constraint-based hierarchical task networks are an alternative, pointed out by \citet{modellite},  which avoid specifying all preconditions and effects through methods and constraints that correspond to underlying, implicit causal links.

As previously stated, this work is a natural extension of the \citet{Garland02} model for evaluating plans in incomplete domains.  Our methods for computing plan failure explanations are slightly different in that we compute them in the forward direction and allow for multiple, interacting faults instead of the single faults.  In addition to calculating the failure explanations of partial plans, we have also presented a relaxed planning heuristic informed by failure explanations.

Prior work of \citet{DBLP:conf/aips/ChangA06} addresses planning with incomplete models, but does not attempt to synthesize robust plans, which is similar to our \FFRISKY{}-$FF$ planner.  We have shown that incorporating knowledge about domain incompleteness into the planner can lead to a more effective agent.  We also differ in that we do not assume direct feedback from the environment about action failures and we can learn action preconditions.

\section{Conclusion}

We have presented the first work to address planning in incomplete domains as
heuristic search to find robust plans.  Our planner, \FFRISKY{}, i) performs
forward search while maintaining plan failure explanations, and ii) estimates
the future failures by propagating failure explanations on planning graphs.  We
have shown that, compared to a planner that essentially ignores aspects of the
incomplete domain, \FFRISKY{} is able to scale reasonably well but find much
better quality plans.  We have also shown that representing plan failure
explanations with prime implicants leads to better scalability than counting
OBDDs models.  Our agent \goalie{} is  effective when using \FFRISKY{}, can learn  incomplete
actions, and can diagnose future and past failures.

%Future work on this topic will focus on additional heuristics for estimating fault that incorporate possible clobberer faults and other negative interactions.  One direction for extending the heuristics will use interaction propagation to measure the cost added by possible or real mutexes \citep{interaction}.  We also intend to compare against the approach mentioned above that translates the incomplete action descriptions into incomplete states and uses a conformant planner.  Directions for extending our model of incompleteness include adding probabilistic measures of various action features existing in the true domain and how incompleteness can be integrated with uncertain/stochastic action effects.

\und{Acknowledgements} This work was supported by DARPA contract HR001-07-C-0060.
%%\footnotesize
%\def\baselinestretch{0.7}
\bibliography{jared}
\bibliographystyle{aaai}
 
\end{document}
